{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Oligopool Calculator` in Action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author** Ayaan Hossain\n",
    "\n",
    "**Updated** January 25, 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `jupyter` notebook will demonstrate the usage of `Oligopool Calculator` and describe its many functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Notebook Setup](#Notebook-Setup)\n",
    "* [What is Oligopool Calculator?](#What-is-Oligopool-Calculator?)\n",
    "* [Design Mode Walkthrough](#Design-Mode-Walkthrough)\n",
    "    * [Simulating the problem](#Simulating-the-problem)\n",
    "    * [Reviewing constraints and formulating gameplan](#Reviewing-constraints-and-formulating-gameplan)\n",
    "    * [Primer design review](#Primer-design-review)\n",
    "    * [Storing the plasmid background](#Storing-the-plasmid-background)\n",
    "    * [Designing the triple-primer system](#Designing-the-triple-primer-system)\n",
    "    * [Generating orthogonal barcodes](#Generating-orthogonal-barcodes)\n",
    "    * [Adding neutral motifs, spacers, and finalizing library](#Adding-neutral-motifs,-spacers,-and-finalizing-library)\n",
    "* [Assembly Mode Walkthrough](#Assembly-Mode-Walkthrough)\n",
    "    * [The Challenge: Synthesis Length Limits](#The-Challenge:-Synthesis-Length-Limits)\n",
    "    * [Splitting long oligos into overlapping fragments](#Splitting-long-oligos-into-overlapping-fragments)\n",
    "    * [Padding fragments for Type IIS assembly](#Padding-fragments-for-Type-IIS-assembly)\n",
    "* [Degenerate Mode Walkthrough](#Degenerate-Mode-Walkthrough)\n",
    "    * [The Challenge: Expensive Similar-Sequence Libraries](#The-Challenge:-Expensive-Similar-Sequence-Libraries)\n",
    "    * [Analysis Mode vs Degenerate Mode](#Analysis-Mode-vs-Degenerate-Mode)\n",
    "    * [The Degenerate Mode Pipeline](#The-Degenerate-Mode-Pipeline)\n",
    "    * [Understanding IUPAC Degenerate Codes](#Understanding-IUPAC-Degenerate-Codes)\n",
    "    * [Compressing a library into degenerate oligos](#Compressing-a-library-into-degenerate-oligos)\n",
    "    * [Realistic Example: 500 ML-style variants](#Realistic-Example:-500-ML-style-variants)\n",
    "    * [Expanding degenerate oligos](#Expanding-degenerate-oligos)\n",
    "* [Analysis Mode Walkthrough](#Analysis-Mode-Walkthrough)\n",
    "    * [The Challenge: Extracting Signal from Sequencing Noise](#The-Challenge:-Extracting-Signal-from-Sequencing-Noise)\n",
    "    * [Indexing the barcodes and associated variants](#Indexing-the-barcodes-and-associated-variants)\n",
    "    * [Packing the NGS reads for quantification](#Packing-the-NGS-reads-for-quantification)\n",
    "    * [Overview of counting methods](#Overview-of-counting-methods)\n",
    "    * [`acount` vs `xcount`: Choosing Your Counting Method](#acount-vs-xcount:-Choosing-Your-Counting-Method)\n",
    "    * [Association counting of variants and barcodes](#Association-counting-of-variants-and-barcodes)\n",
    "    * [Combinatorial barcode counting with callback](#Combinatorial-barcode-counting-with-callback)\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to have `Oligopool Calculator` installed to proceed. You can install it by uncommenting and executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install oligopool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import `oligopool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oligopool as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need `numpy` to do some math, and we will use `pandas` as our data container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need `matplotlib`/`seaborn` for plotting, and `pprint` for pretty printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will need `multiprocessing` `Manager` to create a shared dictionary for demonstrating synchronized callback memory during counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're all set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Oligopool Calculator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oligonucleotide pools (oligo pools) have revolutionized synthetic biology by enabling large-scale, cost-effective synthesis of hundreds of thousands of unique, short single-stranded DNA sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These oligo pools facilitate the creation and study of various biological components, including de novo promoters, ribozymes, protein scaffolds, and CRISPR libraries. Massively parallel reporter assays (MPRAs) are utilized to clone and assess the functionality of these variants using high-throughput next-generation sequencing (NGS), which can generate billions of short reads per experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead, advancements are expected in the number and length of oligos, NGS throughput, and reductions in manufacturing costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Oligopool Calculator Workflow](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/workflow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Oligopool Calculator` is a comprehensive tool for designing and analyzing large-scale oligonucleotide pools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **(a)** `Design Mode`, you build an oligo architecture by adding constrained elements (primers, barcodes, motifs/anchors, spacers) and running QC as you go (`lenstat`, `verify`). Many element modules support **Patch Mode** (`patch_mode=True`) to fill only missing values when you append new rows to an already-designed pool.\n",
    "\n",
    "In `Assembly Mode`, you handle libraries longer than the synthesis limit by fragmenting and padding them for downstream assembly (`split`, `pad`).\n",
    "\n",
    "In `Degenerate Mode`, you can compress a library of similar concrete sequences into IUPAC-degenerate oligos for cost-efficient synthesis (`compress`), and optionally expand them back as a correctness check (`expand`). Best for selection assays where you identify winners by sequencing (no barcode counting required).\n",
    "\n",
    "If you prefer CLI-first workflows, the CLI supports YAML config files for both single commands and multi-step pipelines (see `examples/cli-pipeline/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **(b)** `Analysis Mode`, `Oligopool Calculator` facilitates the processing of next-generation sequencing (NGS) data derived from barcoded amplicons. This mode allows for efficient mapping of variants and their associated barcodes, producing count matrices for quantitative assessment of variant functionality. Two types of counting operations are supported: Association Counting for assessing synthesis accuracy, and Combinatorial Counting for analyzing multiple barcode combinations from sequencing reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall `Oligopool Calculator` provides synthetic biologists with a unified platform for oligo pool design and characterization, addressing the need for a standardized approach across different projects and labs. This tool eliminates the need for reinventing design scripts or using suboptimal strategies, making it particularly valuable for designing and analyzing pools containing millions of defined variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Mode Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to design and test a library of 6,232 ribozyme variants based on the following architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ribozyme Library Architecture](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/architecture-ribozyme.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To characterize the activity of our library in _E. coli_, we adopt a two-barcode strategy. Because ribozymes are self-cleaving, the two barcodes (BC1 and BC2), placed before and after the core variant, are separated after catalysis. This lets us quantify cleavage by counting the ratio of 'separated' to 'unseparated' reads.\n",
    "\n",
    "When using multiple barcode columns, it can also be useful to enforce *cross-set* separation (e.g., ensure BC2 barcodes are also far from the BC1 barcode set), which we will demonstrate later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of demonstration, let us first generate a random library of 59-73 mers (designing exact ribozymes is beyond the scope here) and assume that the library will be cloned into a 5 kb plasmid backbone for characterization, which we will also generate randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = ['A', 'T', 'G', 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribozymes = []\n",
    "while len(ribozymes) < 6232:\n",
    "    variant = ''.join(rng.choice(dna, size=rng.integers(low=59, high=74)))\n",
    "    if 'GAATTC' in variant or 'GACGTC' in variant or 'GGTCTC' in variant or 'GAGACC' in variant:\n",
    "        # Skipping if random variant has EcoRI, AatII, or BsaI motif\n",
    "        continue\n",
    "    ribozymes.append(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(20,5))\n",
    "sns.histplot([len(x) for x in ribozymes], binwidth=1)\n",
    "ax.set_xticks(np.arange(59,74))\n",
    "ax.set_xlabel('Ribozyme Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ribozymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasmid = ''.join(rng.choice(dna) for _ in range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plasmid) # 5 kbp long plasmid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing constraints and formulating gameplan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated 6,232 simulated candidates between 59 to 73 bp in length, and a plasmid of length 5 kbp. We want to use `EcoRI` and `AatII` as our restriction sites as part of the characterization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `Oligopool Calculator` to design the following.\n",
    "* Three primers, each 20bp in length,\n",
    "* The prefix and suffix barcodes each 11bp in length, and\n",
    "* Variable length spacers ranging from 3 to 17bp to pad the oligos to 170bp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As constraints, we aim to have the following.\n",
    "* The primers should not have off-target binding within the library or to the backbone,\n",
    "* Addition of primers, barcodes or spacers should not contain `EcoRI`, `AatII`, or `BsaI` (Type IIS for padding) motifs or introduce them at the edges, and\n",
    "* The barcodes should all have at least 3 mismatches separating each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other realistic constraints, such as exclusion of all palindromic hexamers, or polymeric runs such as `'AAAAA'`, ..., `'CCCCC'` may be included as they were in the original projects, but we will skip those for this demonstration. In reality, constraints will depend on the design project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful habit in Design Mode is to measure and QC as we go:\n",
    "\n",
    "- `lenstat` is a fast length ruler: it reports per-element and per-oligo lengths and remaining free space under an `oligo_length_limit` (it does not modify the input DataFrame).\n",
    "- `verify` is a broader QC pass (often right before synthesis): it summarizes sequence vs metadata columns, flags degenerate/mixed columns, and checks length overflow + excluded motif emergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we begin? By asking `help` from `Oligopool Calculator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the help, we will need to primarily use the `barcode`, `primer`, and `spacer` modules to add the library elements.\n",
    "\n",
    "These element modules (`barcode`, `primer`, `motif`, `spacer`) also support **Patch Mode** (`patch_mode=True`), which is designed for iterative workflows:\n",
    "\n",
    "- If you **append new rows** to an already-designed pool (e.g., you already ordered BC1/primers, then later add new variants), Patch Mode fills only the missing values in an existing output column and leaves existing designs untouched.\n",
    "- This makes **vertical expansion** safe: you can extend a pool without redesigning (and potentially changing) existing barcodes/primers/anchors/spacers.\n",
    "\n",
    "In Patch Mode, missing values (e.g., `None`/NaN/empty/`'-'`) are treated as design targets; existing values are sanity-checked (DNA + expected length) but are not re-designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer design review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, primers are the most constrained elements in an oligo pool library because they have to follow a desired sequence constraint (such as `5'-GC` clamps), be free of inhibitory structures, be very specific to the library and have minimal off-targeting to background. We recommend designing primers as early as possible, but after all other elements in the library have been finalized, such as any motifs (discussed later in this notebook).\n",
    "\n",
    "For multiplexed libraries, you can assign per-oligo set labels via `oligo_sets` to design primers independently for each set while ensuring cross-set compatibility. If a paired primer column is supplied, the paired primer is interpreted per set so that each group remains internally matched.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Primer Design](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/primer.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any step in the process if we get stuck, `Oligopool Calculator` will tell us what is preventing it from succeeding and we can relax or modify our constraints accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we design primers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things should stand out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, primer design involves consideration of background sequences to minimize off-target amplification, such as from the plasmid we set up. The same background can also be reused by other Design Mode modules (`barcode`, `motif`, `spacer`) and by `verify` via `background_directory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, primer design involves consideration of pairing information. If we only had two primers (one forward and one reverse) the order of their design would not matter. But in our problem, there are two forward primers (pink and orange) both of which are coupled to a single reverse primer (yellow). It will be important to design them in the right order, as we will discuss shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first solve the background problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the plasmid background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Oligopool Calculator` makes it easy to define a reusable background k-mer database via the `background` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you want to modify the background post-hoc, or want to define your background using a custom strategy, directly use the `vectorDB` module, which behaves like a dictionary. For details use `help(op.vectorDB)`. The `background` module just wraps around it and offers an one-time use simple interface to `vectorDB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_directory = 'demo.oligopool.background' # Path to store our background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the background if one exists already, for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $background_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the `background` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_stats = op.background(\n",
    "    input_data=plasmid,\n",
    "    maximum_repeat_length=8,\n",
    "    output_directory=background_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(background_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now stored the background, and are ready to design the three primers (and any other elements you want to screen against the same background)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple background DBs (optional)\n",
    "\n",
    "All Design Mode modules that accept `background_directory` also accept multiple backgrounds via a list. If you pass multiple background DBs, designed elements are screened against ALL of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (not executed here):\n",
    "background_directories = [\n",
    "    'demo.oligopool.background',\n",
    "    'ecoli_bg',  # resolves to ecoli_bg.oligopool.background\n",
    "]\n",
    "\n",
    "# df, stats = op.primer(..., background_directory=background_directories)\n",
    "# df, stats = op.barcode(..., background_directory=background_directories)\n",
    "# df, stats = op.motif(..., background_directory=background_directories)\n",
    "# df, stats = op.spacer(..., background_directory=background_directories)\n",
    "# stats = op.verify(..., background_directory=background_directories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: All `Oligopool Calculator` functions should return a pipeline statistics dictionary recapitulating the output from the last step performed in the function. If the operation completed successfully, the `status` key will be marked as `True` and `basis` will be marked as `'solved'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing the triple-primer system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `Design Mode` input and output is mediated through a `pandas` `DataFrame` to facilitate carefully piplined designs. Let us store the variant candidates in a dataframe and start the design process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'ID': [f'ribozyme_{x+1:04d}' for x in range(len(ribozymes))],\n",
    "    'EcoRI': 'GAATTC',\n",
    "    'Variant':ribozymes,\n",
    "    'AatII': 'GACGTC',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the primers are coupled, an optimal solution here is to design the innermost (pink) forward primer first, and then design the reverse primer (yellow), followed by designing the outermost (orange) forward primer. This allows all three primers to be compatible with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate solution would involve designing the (yellow) reverse primer first, and then designing the two forward primers successively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we designed the (yellow) reverse primer at the end, it would be difficult to ensure that it is compatible with both forward primers simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? op.primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, stats = op.primer(\n",
    "    input_data=df,\n",
    "    oligo_length_limit=170,                           # Must not exceed 170bp at any stage\n",
    "    primer_sequence_constraint='SS' + 'N'*18,         # Manually install 5p GC clamp if required\n",
    "    primer_type='forward',                            # Forward primer design\n",
    "    primer_column='PinkForwardPrimer',                # Store designs in 'PinkForwardPrimer' column\n",
    "    left_context_column='Variant',                    # Next to 'Variant' column\n",
    "    right_context_column='AatII',                     # Before 'AatII' column\n",
    "    paired_primer_column=None,                        # No paired primer yet\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'],  # Exclude EcoRI, AatII, BsaI motifs\n",
    "    minimum_melting_temperature=53,                   # Tm = 53C at least\n",
    "    maximum_melting_temperature=55,                   # Tm = 55C at most\n",
    "    maximum_repeat_length=10,                         # No more than 10-mer repeats allowed\n",
    "    background_directory=background_directory,        # As setup before\n",
    "    output_file=None)                                 # In-memory return, no CSV written here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The designed primer is predicted to be free from hairpin structures with low homodimer probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now design the two remaining primers in silent mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = op.primer(\n",
    "    input_data=df,                                   # Updated DataFrame along pipeline chain\n",
    "    oligo_length_limit=170,\n",
    "    primer_sequence_constraint='N'*18 + 'WW',        # Manually install 3p AT clamp if needed\n",
    "    primer_type='reverse',                           # Reverse primer design\n",
    "    primer_column='YellowReversePrimer',             # Store designs in 'YellowReversePrimer' column\n",
    "    left_context_column='PinkForwardPrimer',         # Next to 'PinkForwardPrimer' column\n",
    "    right_context_column='AatII',                    # Before 'AatII' column\n",
    "    paired_primer_column='PinkForwardPrimer',        # Yellow reverse primer paired with pink forward primer\n",
    "    minimum_melting_temperature=53,\n",
    "    maximum_melting_temperature=55,\n",
    "    maximum_repeat_length=10,\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'],  # Exclude EcoRI, AatII, BsaI motifs\n",
    "    background_directory=background_directory,\n",
    "    verbose=False)                               # Silent Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = op.primer(\n",
    "    input_data=df,                                    # Updated DataFrame along pipeline chain\n",
    "    oligo_length_limit=170,\n",
    "    primer_sequence_constraint='N'*20,\n",
    "    primer_type='forward',                            # Forward primer design\n",
    "    primer_column='OrangeForwardPrimer',              # Store designs in 'OrangeForwardPrimer' column\n",
    "    left_context_column='EcoRI',                      # Next to 'EcoRI' column\n",
    "    right_context_column='Variant',                   # Before 'Variant' column\n",
    "    paired_primer_column='YellowReversePrimer',       # Orange forward primer paired with yellow reverse primer\n",
    "    minimum_melting_temperature=53,\n",
    "    maximum_melting_temperature=55,\n",
    "    maximum_repeat_length=10,\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'],  # Exclude EcoRI, AatII, BsaI motifs\n",
    "    background_directory=background_directory,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our primers are installed. Barcodes up next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, since we're done with the background for this demo, let's drop it. In real projects you typically keep the background DB around and reuse it across multiple design steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(op.vectorDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.vectorDB(background_directory, maximum_repeat_length=None).drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating orthogonal barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barcodes are critical elements that help identify each variant uniquely, especially when the core oligo variants are similar to each other. `Oligopool Calculator` maximizes barcode distinguishability which is useful, for example, when the library is exploring all single substitutions to a template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Barcode Design](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/barcode.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.barcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike primers which need to satisfy global constraints (a hard problem), barcodes are optimized contextually. So, barcodes may be designed after all the primers have been installed. However, the pairwise Hamming distance requirement coupled with other constraints makes it difficult to judge ahead of time if there are enough barcodes possible for a given library with desired constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us design BC1 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = op.barcode(\n",
    "    input_data=df,\n",
    "    oligo_length_limit=170,\n",
    "    barcode_length=11,\n",
    "    minimum_hamming_distance=3,  # As defined by our goal\n",
    "    maximum_repeat_length=6,     # To prevent internal hairpins, say\n",
    "    barcode_column='BC1',\n",
    "    barcode_type='spectrum',     # Spectrum optimized barcodes\n",
    "    left_context_column='OrangeForwardPrimer',\n",
    "    right_context_column='Variant',\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'])  # Exclude EcoRI, AatII, BsaI motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the barcodes have at least 4 mismatches separating them, and all of them are at least 3 mismatches apart as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add BC2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-barcode sets (optional):** When designing multiple barcode columns (BC1, BC2, ...), you may want each new barcode set to be separated not only within itself (`minimum_hamming_distance`), but also from existing barcode sets.\n",
    "\n",
    "`cross_barcode_columns` + `minimum_cross_distance` enforce a *global* cross-set Hamming distance between each newly designed barcode and the union of barcodes in the specified column(s) (not a per-row pairing constraint).\n",
    "\n",
    "Below, BC2 is designed to be >=3 mismatches away from every BC1 sequence. For a third barcode, you could use `cross_barcode_columns=['BC1','BC2']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = op.barcode(\n",
    "    input_data=df,\n",
    "    oligo_length_limit=170,\n",
    "    barcode_length=11,\n",
    "    minimum_hamming_distance=3,\n",
    "    maximum_repeat_length=6,\n",
    "    barcode_column='BC2',\n",
    "    barcode_type='spectrum',     # Spectrum optimized barcodes\n",
    "    left_context_column='PinkForwardPrimer',\n",
    "    right_context_column='YellowReversePrimer',\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'],  # Exclude EcoRI, AatII, BsaI motifs\n",
    "    cross_barcode_columns=['BC1'],\n",
    "    minimum_cross_distance=3,\n",
    "    verbose=False)  # Silent mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barcodes are done! The only thing left to design are the spacers so that all oligos have the same length of 170 bp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before designing spacers, we first measure how much length budget is left. The `lenstat` module is a quick ruler: it reports per-element lengths and remaining free space, without modifying the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(op.lenstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stats = op.lenstat(input_data=df, oligo_length_limit=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding neutral motifs, spacers, and finalizing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last element in our oligo is an `AatII` restriction site, and we want to add 3 to 17 bases of filler next to it. We can do this using the `spacer` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But suppose we wanted to add a fixed length spacer site based on a degenerate sequence constraint, such as a motif? We would use the `motif` module instead. One way to think about this is that spacers are a more general form of motifs -- motifs are more constrained in terms of sequence composition and are of fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important use of the `motif` module is in designing constant barcode anchors that is distinct from the remainder of the oligos (controlled via `maximum_repeat_length` parameter) to facilate downstream counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.motif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see this function in action by first removing the `AatII` motif column and replacing it with a degenerate motif containing the constant restriction site embedded in it but with 3 `'N'`s on its right flank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['AatII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = op.motif(\n",
    "    input_data=df,\n",
    "    oligo_length_limit=170,\n",
    "    motif_sequence_constraint='GACGTC'+'NNN', # The AatII motif with right flanking Ns\n",
    "    maximum_repeat_length=6,\n",
    "    motif_column='AatIIPadded',\n",
    "    left_context_column='YellowReversePrimer',\n",
    "    right_context_column=None,\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'])  # Exclude EcoRI, AatII, BsaI motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 in the `motif` pipeline warned us about the presence of `'GACGTC'` in our constraint itself. Nevertheless, the design step was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stats = op.lenstat(input_data=df, oligo_length_limit=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `spacer` module to pad the oligos to 170 bases. Free space available is between 0 to 14 bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.spacer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Oligopool Calculator` can flexibly design our variable length spacers. Here our aim is to add a spacer so that the final oligo length is 170 bases, and we can achieve this by simply specifying the `oligo_length_limit` and using `None` as our `spacer_length` argument. Otherwise, we could specify the length of each spacer, in order, per oligo in a list or a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = op.spacer(\n",
    "    input_data=df,\n",
    "    oligo_length_limit=170,\n",
    "    spacer_length=None,    # Using None, we will automatically add spacers to match oligo_length_limit\n",
    "    maximum_repeat_length=6,\n",
    "    spacer_column='Spacer',\n",
    "    left_context_column='AatIIPadded',\n",
    "    right_context_column=None,\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'],  # Exclude EcoRI, AatII, BsaI motifs\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stats = op.lenstat(input_data=df, oligo_length_limit=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our library is complete, all elements have been added, and the oligos are all 170 bases. Let's finalize our library using the `final` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? op.final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df, _ = op.final(input_data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** All annotations of the individual oligos are lost in this step. So, we saved it to a new DataFrame. In general, it is a good idea to store the result of each step in `Design Mode` to separate DataFrame variables if we were designing our pools interactively, and wanted to roll back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `final` module produces a DataFrame that needs to be passed along for synthesis. `Design Mode` functions should no longer be applied on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before shipping the library for synthesis, it is useful to run a quick QC pass. Compared to `lenstat`, the `verify` module is broader: it summarizes the architecture and length distribution, inspects column types (sequence vs metadata), and checks that excluded motifs did not re-emerge beyond their intended occurrence.\n",
    "\n",
    "It also flags degenerate/IUPAC sequence columns (e.g., UMIs) so you can confirm they are intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_stats = op.verify(\n",
    "    input_data=final_df,\n",
    "    oligo_length_limit=170,\n",
    "    excluded_motifs=['GAATTC', 'GACGTC', 'GGTCTC', 'GAGACC'],  # Exclude EcoRI, AatII, BsaI motifs\n",
    "    verbose=False)\n",
    "verify_stats['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly check for any violation of our motif constraints. For example, in all our designed oligos there should only be single occurrences of the `EcoRI` and `AatII` motifs at their respective location, and no occurrences of the `BsaI` motif (since it will be used for pad excision later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert final_df.CompleteOligo.transform(lambda x: x.count('GAATTC')).values.max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert final_df.CompleteOligo.transform(lambda x: x.count('GACGTC')).values.max() == 1\n",
    "assert final_df.CompleteOligo.transform(lambda x: x.count('GGTCTC')).values.max() == 0  # No BsaI sites\n",
    "assert final_df.CompleteOligo.transform(lambda x: x.count('GAGACC')).values.max() == 0  # No BsaI RC sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our `Oligopool Calculator` `Design Mode` walkthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembly Mode Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge: Synthesis Length Limits\n",
    "\n",
    "Modern oligo synthesis has practical length limits - typically 150-300 bp depending on provider and chemistry. But many experimental designs require longer constructs:\n",
    "\n",
    "**Common scenarios requiring assembly:**\n",
    "- Multi-domain protein libraries (promoter + RBS + CDS + terminator)\n",
    "- Long regulatory elements (enhancers spanning 500+ bp)\n",
    "- Combinatorial screening with multiple functional units\n",
    "- CRISPR arrays with multiple guide RNAs\n",
    "\n",
    "**The solution:** Fragment your design into overlapping pieces that can be synthesized separately, then assembled via overlap-extension PCR (or similar overlap-based assembly). `Assembly Mode` provides two functions:\n",
    "- **`split`**: Fragments long oligos into overlapping pieces with optimized assembly junctions\n",
    "- **`pad`**: Adds Type IIS restriction sites for scarless excision after amplification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting long oligos into overlapping fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split` function fragments long oligos into overlapping pieces optimized for assembly:\n",
    "- Overlaps are designed with compatible melting temperatures for PCR assembly\n",
    "- Hamming distance constraints prevent cross-assembly between fragments from different oligos\n",
    "- Fragments are returned in alternating strand orientations (ready for PCR)\n",
    "\n",
    "Treat each `SplitN` column as a separate synthesis pool: run `pad` once per split column, then `final` each padded DataFrame. If you prefer one DataFrame per fragment, enable `separate_outputs` (CLI `op split` writes separate files by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Splitting and Padding](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/assembly.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pretend that our 170 bp oligos from above are slightly longer than can be synthesized successfully in a pool where each oligo can be at most 150 bp long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal then is to fragment the oligos so that each fragment is at most 110 bases with 40 to 60 base overlap across the fragments. We will leave 40 bases, 20 bp each for the two terminal padding primers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df, _ = op.split(\n",
    "    input_data=df,          # Notice, we are not using the 'final_df'\n",
    "    split_length_limit=110, # Each split must be at most 110 bases long\n",
    "    minimum_melting_temperature=55, # Assembly Tm is 55C\n",
    "    minimum_hamming_distance=5,     # Ensure the overlaps are distinct\n",
    "    minimum_overlap_length=40,    # At least 40 bp overlap\n",
    "    maximum_overlap_length=60)    # At most 60 bp overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stats = op.lenstat(input_data=split_df, oligo_length_limit=150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have automatically split our library for two-fragment assembly. The first pool contains 110 bp oligos while the oligos in the second pool are shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split fragments are in PCR assembly order, meaning the first fragment and the second fragments are returned in opposite strands, so that PCR generates the correct full-length double-stranded oligos. We can verify this using the `revcomp` module to change the orientation of `'Split2'` column and see that the ends of `'Split1'` sequences overlap with the beginning of the sequences in `'Split2'` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip (not demoed):** Use `merge` to collapse a column range into a single DNA element before other operations.\n",
    "\n",
    "This is helpful when a multi-column region (e.g., `ForwardPrimer` + `BC1` + `Variant` + `BC2`) should be treated as one contiguous sequence for downstream steps like `revcomp` or `final`.\n",
    "\n",
    "Learn more: run `help(op.merge)` (or `? op.merge`) in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(op.revcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df, _ = op.revcomp(\n",
    "    input_data=split_df,\n",
    "    left_context_column='Split2',\n",
    "    right_context_column='Split2',\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = check_df.head(1).Split1.values[0]\n",
    "split2 = check_df.head(1).Split2.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(len(split1), len(split2)), 0, -1):\n",
    "    if split1[-i:] == split2[:i]:\n",
    "        alignment_padding = len(split1) - i\n",
    "        print(split1)\n",
    "        print(' ' * alignment_padding + '|' * i)\n",
    "        print(' ' * alignment_padding + split2)\n",
    "        break\n",
    "else:\n",
    "    print('No overlap between computed splits!')\n",
    "\n",
    "# Verify overlap exists\n",
    "assert any(split1[-i:] == split2[:i] for i in range(max(len(split1), len(split2)), 0, -1)), \\\n",
    "    'Split fragments should have valid overlap for assembly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding fragments for Type IIS assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pad` module allows designing of padded primers with embedded Type IIS restriction sites to amplify the oligo fragments, convert them to double-stranded DNA (assuming synthesized oligos are ssDNA) and enable scarless assembly downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will design our pads with `'BsaI'`, as an example. Note that we excluded the BsaI recognition motif (`GGTCTC` and its reverse complement `GAGACC`) from all earlier design elements (primers, barcodes, motifs, spacers) to ensure the Type IIS enzyme only cuts at the designed pad excision sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pad_df, _ = op.pad(\n",
    "    input_data=split_df,\n",
    "    oligo_length_limit=150, # Recall, we are pretending 150 bp synthesis limit\n",
    "    split_column='Split1',  # We will pad the first fragment\n",
    "    typeIIS_system='BsaI',  # Pad with internal 3' BsaI motif\n",
    "    minimum_melting_temperature=53,\n",
    "    maximum_melting_temperature=55,\n",
    "    maximum_repeat_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.lenstat(input_data=first_pad_df, oligo_length_limit=150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. On to the second padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pad_df, _ = op.pad(\n",
    "    input_data=split_df,\n",
    "    oligo_length_limit=150,\n",
    "    split_column='Split2',  # We will pad the second fragment now\n",
    "    typeIIS_system='BsaI',\n",
    "    minimum_melting_temperature=53,\n",
    "    maximum_melting_temperature=55,\n",
    "    maximum_repeat_length=10,\n",
    "    verbose=False)  # Silent Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pad_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.lenstat(input_data=second_pad_df, oligo_length_limit=150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our `Oligopool Calculator` `Assembly Mode` walkthrough.\n",
    "\n",
    "**Key takeaways:**\n",
    "- `split` fragments long oligos into overlapping pieces with Tm-optimized junctions\n",
    "- Overlaps are Hamming-separated to prevent cross-assembly between different oligos\n",
    "- `pad` adds Type IIS restriction sites for scarless excision after PCR amplification\n",
    "- Run `pad` and `final` separately for each fragment pool before ordering synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degenerate Mode Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge: Expensive Similar-Sequence Libraries\n",
    "\n",
    "Modern protein engineering increasingly relies on machine learning to predict optimal sequences. A typical workflow generates thousands of candidate variants - often highly similar sequences clustered around a predicted optimum.\n",
    "\n",
    "**The cost problem:** Ordering 10,000 concrete oligos at \\$0.10/base x 150 bp = **\\$150,000** for synthesis alone. If those variants compress to 500 degenerate oligos, synthesis cost drops to **\\$7,500** - a 20x reduction.\n",
    "\n",
    "**The experiment design insight:** For many experiments (growth selection, FACS sorting, survival screens), you don't need to *count* every variant. Selection naturally enriches winners. You only need to *identify* which sequences survive.\n",
    "\n",
    "`Degenerate Mode` addresses this by compressing similar concrete sequences into IUPAC-degenerate oligos. The compression is **lossless** - expanding the degenerate oligos recovers exactly your original variant set, with no invented sequences.\n",
    "\n",
    "**Note:** Barcoded libraries typically compress poorly (barcodes are intentionally diverse), so Degenerate Mode is mainly used for selection assays where you identify winners by sequencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Mode vs Degenerate Mode\n",
    "\n",
    "| | **Analysis Mode** | **Degenerate Mode** |\n",
    "|---|---|---|\n",
    "| **Goal** | Quantify all variants | Find top performers |\n",
    "| **Readout** | Barcode abundance | Survivor identity |\n",
    "| **Synthesis format** | Concrete oligos (one per variant) | IUPAC-degenerate oligos |\n",
    "| **Cost scaling** | Linear with variants | Sublinear (compression) |\n",
    "| **Analysis pipeline** | `index` -> `pack` -> `acount`/`xcount` | Sequence survivors + align |\n",
    "| **Best for** | MPRA, expression maps, dose-response | Optimization, directed evolution, screening |\n",
    "\n",
    "**When to use Degenerate Mode:**\n",
    "- Libraries with similar sequences (good compression potential)\n",
    "- Directed evolution or optimization screens\n",
    "- Growth-based, FACS-based, or survival selection assays\n",
    "- Cost-sensitive experiments with large variant pools\n",
    "- Experiments where you can identify winners by sequencing (no barcode counting)\n",
    "\n",
    "**Stick with Analysis Mode when:**\n",
    "- You need quantitative abundance measurements for every variant\n",
    "- Variants are highly dissimilar (poor compression, minimal savings)\n",
    "- You need barcode-based tracking through complex workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Degenerate Mode Pipeline\n",
    "\n",
    "The Degenerate Mode workflow integrates seamlessly with Design Mode. Start by designing your library with any Design Mode methods as required and `verify` all constraints, then replace the final `final` step with `compress`. This produces a smaller set of IUPAC-degenerate oligos that encode all your variants.\n",
    "\n",
    "After synthesis, amplify and clone the degenerate pool, then run your selection assay (growth, FACS, survival screen). The selection naturally enriches winners. Finally, sequence the survivors and use `mapping_df` to trace each surviving sequence back to its original variant ID(s).\n",
    "\n",
    "The key insight: you don't need barcodes or counting infrastructure. Selection does the work, and `mapping_df` provides traceability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding IUPAC Degenerate Codes\n",
    "\n",
    "IUPAC codes extend the standard A/T/G/C alphabet to represent positions that can be any subset of bases. Synthesis vendors interpret these codes and produce a mixture of oligos covering all combinations.\n",
    "\n",
    "| Code | Name | Bases | Degeneracy |\n",
    "|:---:|:---|:---:|:---:|\n",
    "| A | Adenine | A | 1 |\n",
    "| C | Cytosine | C | 1 |\n",
    "| G | Guanine | G | 1 |\n",
    "| T | Thymine | T | 1 |\n",
    "| R | Purine | A, G | 2 |\n",
    "| Y | Pyrimidine | C, T | 2 |\n",
    "| S | Strong | G, C | 2 |\n",
    "| W | Weak | A, T | 2 |\n",
    "| K | Keto | G, T | 2 |\n",
    "| M | Amino | A, C | 2 |\n",
    "| B | Not A | C, G, T | 3 |\n",
    "| D | Not C | A, G, T | 3 |\n",
    "| H | Not G | A, C, T | 3 |\n",
    "| V | Not T | A, C, G | 3 |\n",
    "| N | Any | A, C, G, T | 4 |\n",
    "\n",
    "**Example:** The sequence `ATGCNATGC` has degeneracy 4 (one `N` position x 4 bases). It represents the set: `{ATGCAATGC, ATGCCATGC, ATGCGATGC, ATGCTATGC}`.\n",
    "\n",
    "The total degeneracy of a sequence is the product of degeneracies at each position. A sequence with two `N` positions has degeneracy 4 x 4 = 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oligopool Calculator stores IUPAC codes internally as ddna_space\n",
    "# (degenerate DNA space) - a mapping from code to the set of bases it represents\n",
    "from oligopool.base.utils import ddna_space\n",
    "\n",
    "# Show the mapping (excluding '-' placeholder)\n",
    "for code, bases in ddna_space.items():\n",
    "    if code != '-':\n",
    "        print(f'{code}: {sorted(bases)} (degeneracy {len(bases)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compressing a library into degenerate oligos\n",
    "\n",
    "Let's see how `compress` works with a simple example first, then apply it to a more realistic ML-style library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy Example: 12 variants -> ? degenerate oligos**\n",
    "\n",
    "Let's create a small library where some sequences differ only at a few positions. This is representative of libraries with low mutational diversity where variants cluster around similar sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 variants: two clusters of similar sequences\n",
    "# Cluster 1: ATGCATGC... with variations at positions 9, 10, 11\n",
    "# Cluster 2: GCTAGCTA... with variations at positions 10, 11, 12\n",
    "toy_df = pd.DataFrame({\n",
    "    'ID': [f'V{i:03d}' for i in range(1, 13)],\n",
    "    'Sequence': [\n",
    "        # Cluster 1: positions 9-12 vary (last 4 bases)\n",
    "        'ATGCATGCATGC',  # V001\n",
    "        'ATGCATGCATGT',  # V002\n",
    "        'ATGCATGCATGA',  # V003\n",
    "        'ATGCATGCATGG',  # V004\n",
    "        'ATGCATGCCTGC',  # V005: also varies at position 9\n",
    "        'ATGCATGCGTGC',  # V006\n",
    "        'ATGCATGCTTGC',  # V007\n",
    "        'ATGCATGCCCGC',  # V008\n",
    "        # Cluster 2: different prefix, varies at last 2-3 bases\n",
    "        'GCTAGCTAGCTA',  # V009\n",
    "        'GCTAGCTAGCTT',  # V010\n",
    "        'GCTAGCTAGCTG',  # V011\n",
    "        'GCTAGCTAGCAA',  # V012: 2 positions different\n",
    "    ],\n",
    "})\n",
    "print(f'Input: {len(toy_df)} concrete variants')\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the toy library\n",
    "# rollout_simulations and rollout_horizon control the Monte Carlo search\n",
    "# Higher values = better compression but slower runtime\n",
    "toy_mapping_df, toy_synthesis_df, toy_stats = op.compress(\n",
    "    input_data=toy_df,\n",
    "    rollout_simulations=20,   # Monte Carlo rollouts per decision\n",
    "    rollout_horizon=3,        # Lookahead positions\n",
    "    random_seed=42,           # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compress` returns **three outputs**:\n",
    "\n",
    "1. **`mapping_df`**: Maps each original variant ID to its concrete sequence and the degenerate oligo ID that covers it. This is your traceability map for post-selection analysis.\n",
    "\n",
    "2. **`synthesis_df`**: The degenerate oligos to order from your synthesis vendor. Each row is one oligo with its IUPAC sequence, degeneracy (how many concrete sequences it represents), and length.\n",
    "\n",
    "3. **`stats`**: Compression statistics including compression ratio, degeneracy distribution, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_df: your lookup table for post-selection identification\n",
    "# Each variant ID maps to its concrete sequence and degenerate oligo ID\n",
    "print('Mapping DataFrame (variant -> degenerate oligo):')\n",
    "display(toy_mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesis_df: what you actually order from the vendor\n",
    "# Each degenerate oligo encodes multiple concrete variants\n",
    "print('Synthesis DataFrame (order sheet):')\n",
    "display(toy_synthesis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nCompression: {len(toy_df)} variants -> {len(toy_synthesis_df)} degenerate oligos '\n",
    "      f'({len(toy_df)/len(toy_synthesis_df):.1f}x compression)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression statistics\n",
    "pprint(toy_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realistic Example: 500 ML-style variants\n",
    "\n",
    "Now let's simulate a more realistic scenario: an ML model has predicted 500 variants clustered around 5 template sequences, each with 0-3 point mutations. This is typical of protein optimization workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_rng = np.random.default_rng(seed=123)  # Reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 template sequences (40 bp each) - representing ML-predicted optimal regions\n",
    "templates = [\n",
    "    'TTGACAATTAATCATCGAACTAGTTAACTAGTACGCAAG',\n",
    "    'CGTACGATCGATCGATCGATCGATCGATCGATCGATCGA',\n",
    "    'ATGCATGCATGCATGCATGCATGCATGCATGCATGCATGC',\n",
    "    'GCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTA',\n",
    "    'AACCGGTTAACCGGTTAACCGGTTAACCGGTTAACCGGTT',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(seq, n_mutations, rng):\n",
    "    '''Introduce n random point mutations into sequence.'''\n",
    "    seq = list(seq)\n",
    "    positions = rng.choice(len(seq), size=n_mutations, replace=False)\n",
    "    for pos in positions:\n",
    "        bases = ['A', 'T', 'G', 'C']\n",
    "        bases.remove(seq[pos])  # Don't substitute with same base\n",
    "        seq[pos] = rng.choice(bases)\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants: 100 per template, 0-3 mutations each.\n",
    "ml_variants = []\n",
    "for template_idx, template in enumerate(templates):\n",
    "    for i in range(100):\n",
    "        n_mutations = ml_rng.integers(0, 4)  # 0, 1, 2, or 3 mutations\n",
    "        variant = mutate_sequence(template, n_mutations, ml_rng)\n",
    "        ml_variants.append({\n",
    "            'ID': f'ML_{template_idx+1:02d}_{i+1:03d}',\n",
    "            'Sequence': variant,\n",
    "            'Template': f'T{template_idx+1}',\n",
    "            'Mutations': n_mutations\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = pd.DataFrame(ml_variants)\n",
    "print(f'Generated {len(ml_df)} ML-style variants from {len(templates)} templates')\n",
    "print(f'Mutation distribution: {ml_df.Mutations.value_counts().sort_index().to_dict()}')\n",
    "ml_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the ML library (using only ID and Sequence columns)\n",
    "ml_compress_input = ml_df[['ID', 'Sequence']].copy()\n",
    "\n",
    "ml_mapping_df, ml_synthesis_df, ml_stats = op.compress(\n",
    "    input_data=ml_compress_input,\n",
    "    rollout_simulations=50,   # More simulations for better compression\n",
    "    rollout_horizon=4,        # Deeper lookahead\n",
    "    random_seed=42,\n",
    "    verbose=False,            # Silent mode for cleaner output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression results\n",
    "n_variants = len(ml_df)\n",
    "n_degenerate = len(ml_synthesis_df)\n",
    "compression_ratio = n_variants / n_degenerate\n",
    "\n",
    "print(f'Compression Results:')\n",
    "print(f'     Input variants: {n_variants:,}')\n",
    "print(f'  Degenerate oligos: {n_degenerate:,}')\n",
    "print(f'  Compression ratio: {compression_ratio:.1f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cost savings estimate (at $0.10/base x 40 bp):')\n",
    "print(f'  Concrete synthesis:   ${n_variants * 40 * 0.10:,.0f}')\n",
    "print(f'  Degenerate synthesis: ${n_degenerate * 40 * 0.10:,.0f}')\n",
    "print(f'  Savings:              ${(n_variants - n_degenerate) * 40 * 0.10:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding degenerate oligos\n",
    "\n",
    "The `expand` function reverses compression: it takes synthesis_df (degenerate oligos) and returns all concrete sequences they encode. This is useful for:\n",
    "- **Verification**: Confirm that expanding the compressed output recovers exactly the set of unique input sequences (lossless guarantee)\n",
    "- **Reference building**: Generate the full concrete sequence set for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify lossless guarantee: expand(compress(X)) == X\n",
    "# Expand the toy example and check that we get back the original sequences\n",
    "toy_expanded_df, toy_expand_stats = op.expand(\n",
    "    input_data=toy_synthesis_df,\n",
    "    sequence_column='DegenerateSeq',\n",
    "    verbose=False,\n",
    ")\n",
    "toy_expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expanded sequences should exactly match our original input\n",
    "original_sequences = set(toy_df['Sequence'].values)\n",
    "expanded_sequences = set(toy_expanded_df['ExpandedSeq'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original sequences:  {len(original_sequences)}')\n",
    "print(f'Expanded sequences:  {len(expanded_sequences)}')\n",
    "print(f'Match: {original_sequences == expanded_sequences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertion to verify lossless compression\n",
    "assert original_sequences == expanded_sequences, 'Lossless guarantee violated!'\n",
    "print('\\nLossless guarantee verified for toy example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our `Oligopool Calculator` `Degenerate Mode` walkthrough.\n",
    "\n",
    "**Key takeaways:**\n",
    "- `compress` reduces synthesis cost by encoding similar sequences in IUPAC-degenerate oligos\n",
    "- The compression is **lossless**: expanding `synthesis_df` recovers exactly the set of unique input sequences\n",
    "- Use `mapping_df` to trace survivors back to original variant IDs after selection\n",
    "- File outputs use distinct suffixes: `mapping_file` -> `.oligopool.compress.mapping.csv`, `synthesis_file` -> `.oligopool.compress.synthesis.csv`\n",
    "- Best for selection assays: sequence survivors and map winners back to variant IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Mode Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge: Extracting Signal from Sequencing Noise\n",
    "\n",
    "After synthesizing and characterizing your oligo pool library, you need to quantify variant activity from NGS reads. This sounds simple - just count barcodes - but real-world data presents challenges:\n",
    "\n",
    "**The problem:**\n",
    "- Millions of reads with sequencing errors\n",
    "- Barcodes may be partially obscured by errors\n",
    "- Need to verify barcode-variant associations survived cloning\n",
    "- Multiple experimental designs require different counting strategies\n",
    "\n",
    "**The solution:** `Analysis Mode` provides a specialized pipeline optimized for barcoded oligo pool quantification:\n",
    "- **`index`**: Train a `Scry` classifier on your barcode+variant architecture\n",
    "- **`pack`**: QC-filter and deduplicate reads for efficient counting\n",
    "- **`acount`/`xcount`**: Count with error-tolerant matching and optional callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ribozyme Library Architecture](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/architecture-ribozyme.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `Design Mode` walkthrough we designed a pool of over 6,000 simulated ribozyme variants. One characterization approach may involve sequencing an amplicon spanning `OrangeForwardPrimer` to `YellowReversePrimer` for uncleaved products, while a second sequencing of cleaved products spans `PinkForwardPrimer` through `YellowReversePrimer`. The ratio of cleaved to uncleaved products would give us the cleavage rate quantification for a ribozyme variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Oligopool Calculator` `Analysis Mode` provides efficient tools to analyze barcoded read counts to facilitate this step, in the form of `index`, `pack`, `acount` and `xcount` modules. In this section we will explore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing the barcodes and associated variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to `index` the barcodes and associated variants to train a `Scry` barcode classifier for downstream counting. This step is analogous to indexing reference genomes that short-read aligners require before mapping NGS reads for genomic analysis, except our `Analysis Mode` functions are optimized for counting and therefore very sensitive and efficient for this task.\n",
    "\n",
    "**Anchors and multi-anchor reads:** `index` can use constant prefix/suffix anchor columns to localize barcodes (and optional associates) in reads. If an anchor appears multiple times in a read, the counting engine uses the best-scoring placement; ties (ambiguous anchors) are rejected. This is why anchor design (typically via `motif(motif_type=1, ...)` and repeat screening) matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Indexing Reads](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/index.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althrough not explored here, you can learn more about using `Scry` via `help(op.Scry)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we will load an annotated oligo pool we generated in the previous section from which we simulated a naive ribozyme cleavage experiment. The notebooks, reference CSV, and the simulated NGS reads are all located in our `analysis-pipeline` directory inside `examples`. The simulation parameters, in particular, can be adjusted in `read_simulator.ipynb` to test `Analysis Mode` functions with different characterization profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('analysis-pipeline/ribozyme_architecture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now index the two barcode sets, since indexing requires us to index them individually. For BC2 in particular, we will associate the core variants for association counting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo, remove any previous indexes\n",
    "! rm -f BC*.oligopool.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = op.index(\n",
    "    barcode_data=df,\n",
    "    barcode_column='BC1',                        # Index BC1 only\n",
    "    barcode_prefix_column='OrangeForwardPrimer', # constant prefix anchor adjacent to BC1\n",
    "    barcode_suffix_column=None,                  # We have no flanking right constant for BC1\n",
    "    barcode_prefix_gap=0,\n",
    "    barcode_suffix_gap=0,\n",
    "    index_file='BC1',                            # Store results in BC1.oligopool.index file\n",
    "    verbose=False,       # Silent Mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = op.index(\n",
    "    barcode_data=df,\n",
    "    barcode_column='BC2',                        # Index BC2 only\n",
    "    barcode_prefix_column='PinkForwardPrimer',   # constant prefix anchor adjacent to BC2\n",
    "    barcode_suffix_column='YellowReversePrimer', # We have YellowReversePrimer as flanking right constant for BC2\n",
    "    barcode_prefix_gap=0,\n",
    "    barcode_suffix_gap=0,\n",
    "    associate_data=df,                           # Associates are in same DataFrame\n",
    "    associate_column='Variant',\n",
    "    associate_prefix_column=None,\n",
    "    associate_suffix_column='PinkForwardPrimer',\n",
    "    associate_prefix_gap=0,\n",
    "    associate_suffix_gap=0,\n",
    "    index_file='BC2',                            # Store results in BC2.oligopool.index file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** Without specifying the associated variants for a barcode set, the resulting index cannot be used for association counting. Here, the `BC1` index can only be used for combinatorial counting, while `BC2` can be used for both association and combinatorial counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our barcodes and their associates are indexed. Let us now pack the NGS reads for counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packing the NGS reads for quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGS analysis begins with quality control of reads. Only high quality reads should be used for characterization. The `pack` module allows us to filter low quality or incorrect reads so that our quantifcation downstream is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, if the reads are paired-end and long enough, compared to the synthesized oligo, the two mates may need to be merged into a consensus fragment, which can be done within the packing step. Alternatively, efficient tools such as [`flash2`](https://github.com/dstreett/FLASH2) can be used for the same -- merge your reads with `flash2` and use the resulting single-end reads in `pack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, due to the nature of MPRA characterization, there may be an abundance of truly duplicate reads which `pack` module consolidates into read packs for efficient and parallel counting downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you may wish to add [unique molecular indexes](https://www.illumina.com/techniques/sequencing/ngs-library-prep/multiplexing/unique-molecular-identifiers.html) (UMIs) during your library preparation to remove spurious PCR or optical duplicates. We recommend directly processing your reads with tools such as [`calib`](https://github.com/vpc-ccg/calib) to consolidate these UMI duplicates, prior to packing and counting them using `Oligopool Calculator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Packing Reads](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/pack.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous pack, if it exists\n",
    "! rm -f NGS.oligopool.pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = op.pack(\n",
    "    r1_fastq_file='analysis-pipeline/ribozyme_1M_R1.fq.gz',\n",
    "    r2_fastq_file='analysis-pipeline/ribozyme_1M_R2.fq.gz',\n",
    "    r1_read_type='forward',     # R1 is in Forward Orientation\n",
    "    r2_read_type='reverse',     # R2 is in Reverse Orientation\n",
    "    minimum_r1_read_quality=30, # Filter out any reads with Phred Score less than 30\n",
    "    minimum_r2_read_quality=30,\n",
    "    minimum_r1_read_length=10,  # Filter out any reads shorter than 10 bases\n",
    "    minimum_r2_read_length=10,\n",
    "    pack_type='merge',          # R1 and R2 needs to be merged\n",
    "    pack_size=0.1, # Store 100K reads per read pack\n",
    "    pack_file='NGS',\n",
    "    core_count=4,  # Use only 4 CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our reads are packed and we are ready to count the barcodes. We can see that all reads \"survived\" our filters and were merged successfully. Despite sequencing a million reads, we only need to analyze less than 20 thousand reads in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of counting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of the analysis pipeline utilizes packed reads and indexed barcode/variant information to generate a read count matrix, which is essential for tabulating variant activity levels. `Oligopool Calculator` `Analysis Mode` offers two efficient counting functions tailored to experimental needs: association counting and combinatorial counting. Users can define parameters for error tolerance and mapping strategy, and they have the option to specify custom callback functions that execute concurrently with the mapping and counting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Counting Reads](https://raw.githubusercontent.com/ayaanhossain/repfmt/refs/heads/main/oligopool/img/count.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `acount` vs `xcount`: Choosing Your Counting Method\n",
    "\n",
    "| | **acount** (Association) | **xcount** (Combinatorial) |\n",
    "|---|---|---|\n",
    "| **Goal** | Verify barcode-variant pairing | Count barcode combinations |\n",
    "| **Index requirement** | Single index with associates | One or more indexes |\n",
    "| **Validation** | Checks variant sequence match | Maps barcodes independently |\n",
    "| **Output** | `BarcodeCounts` + `AssociationCounts` | `CombinatorialCounts` |\n",
    "| **Best for** | QC synthesis, input library | Activity assays, multi-barcode designs |\n",
    "| **Reads used** | Only if barcode+variant match | If any barcode matches |\n",
    "\n",
    "**Use `acount` when:**\n",
    "- Verifying synthesis fidelity (did we get the designed barcode-variant pairs?)\n",
    "- Quantifying input library distribution before selection\n",
    "- Validating cloning did not scramble associations\n",
    "\n",
    "**Use `xcount` when:**\n",
    "- Counting combinations of multiple barcodes (e.g., cleaved vs uncleaved)\n",
    "- Flexible designs where not all barcodes may be present in every read\n",
    "- Activity assays where you track barcode ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association counting of variants and barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In association counting, the goal is to verify the presence of designed variants uniquely assigned to specific barcodes. This function is useful for confirming oligo pool synthesis as an initial check before cloning, or for quantifying input library distribution, or validating the association between barcodes and variants after modifications, such as replacing a spacer with a reporter protein and verifying the association across the terminal ends. The process involves mapping the core barcode after realigning the read with respect to associate prefix or suffix constants, and verifying the presence of the extracted variant subsequence against the full reference variant sequence. Reads are discarded if associate constants are missing or if the core variant has too many errors and thus cannot be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.acount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous AC output, if it exists\n",
    "! rm -f AC.oligopool.acount.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the association of our ribozyme pool based on BC2 index (BC1 index is not usable here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df, _ = op.acount(\n",
    "    index_file='BC2.oligopool.index', # Map BC2\n",
    "    pack_file='NGS.oligopool.pack',   # in our reads\n",
    "    count_file='AC',\n",
    "    mapping_type='sensitive', # Use sensitive/slow classification\n",
    "    barcode_errors=-1,   # Use model-based auto-inferred error tolerance\n",
    "    associate_errors=-1,\n",
    "    callback=None,\n",
    "    core_count=0,\n",
    "    memory_limit=0.0,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the reads were mis-associated, as in everytime we detected a barcode, its associated variant was also found to be present in the read within error tolerance. This is of course artificial in our simulated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our counts! The `BarcodeCounts` column shows the number of times the barcode was detected in a read, while `AssociationCounts` shows the number of times the barcode was unambiguously present with its designated associated variant.\n",
    "\n",
    "In this simulated case, all reads are error-free, so `AssociationCounts` is the same as `BarcodeCounts`. In real-life scenarios, we expect `AssociationCounts` to be <= `BarcodeCounts` because not all barcoded reads retain an intact associate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the ranks of the designs and make some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df['variant_rank'] = ac_df['BC2.ID'].transform(lambda x: int(x.split('_')[-1]))\n",
    "ac_df['variant_rank_bin'] = ac_df.variant_rank.transform(lambda x: x//1000)\n",
    "ac_df['variant_norm_read_count'] = (ac_df.BarcodeCounts - ac_df.BarcodeCounts.mean()) / ac_df.BarcodeCounts.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(16,7))\n",
    "sns.scatterplot(\n",
    "    data=ac_df,\n",
    "    x='variant_rank', y='BarcodeCounts',\n",
    "    hue='variant_rank_bin',\n",
    "    palette='Paired',\n",
    "    s=5, ec=None,\n",
    "    ax=ax[0]).set(\n",
    "        ylabel='read_count')\n",
    "sns.boxplot(\n",
    "    data=ac_df,\n",
    "    x='variant_norm_read_count',\n",
    "    fill=False, notch=True, linewidth=1,\n",
    "    hue='variant_rank_bin',\n",
    "    palette='Paired',\n",
    "    ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simulated, the higher the rank of the variant, the more frequently it appeared. In real experimental data, if the pool being sequenced has not undergone selection (say input library) then we expect this distribution to be uniform across all variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinatorial barcode counting with callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinatorial counting is employed when a single or multiple barcode combinations are expected within sequenced reads. `xcount` uses multiple indexes to map the barcodes independently, allowing for flexibility in experimental design. Reads are only discarded if none of the expected barcodes are present, meaning that one or more missing barcodes are tolerated. This approach can accommodate isolated barcodes with their own flanking constants or unique combinations of sub-sequences within a larger assembly through combinatorial methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? op.xcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply combinatorial counting using both BC1 and BC2 index here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous CC output, if it exists\n",
    "! rm -f CC.oligopool.xcount.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df, _ = op.xcount(\n",
    "    index_files=['BC1', 'BC2'],     # Map combinations of BC1 and BC2\n",
    "    pack_file='NGS.oligopool.pack',\n",
    "    count_file='CC',\n",
    "    mapping_type='sensitive',       # Use sensitive/slow classification\n",
    "    barcode_errors=-1,\n",
    "    callback=None,\n",
    "    core_count=0,\n",
    "    memory_limit=0.0,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have counts of either BC2 or both BC1 and BC2 barcodes from our reads, signifying cleaved and ucleaved products respectively. We can now use this count matrix to compute the activity of the ribozymes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df['ID'] = cc_df['BC2.ID']\n",
    "cc_df['count_basis'] = cc_df.apply(lambda x: 'cleaved_reads' if x['BC1.ID'] != x['BC2.ID'] else 'uncleaved_reads', axis=1)\n",
    "act_df = cc_df.pivot(index='ID', columns='count_basis', values='CombinatorialCounts').fillna(0.).reset_index()\n",
    "act_df['activity'] = act_df.apply(lambda x: 100. * x.cleaved_reads / (x.cleaved_reads + x.uncleaved_reads), axis=1)\n",
    "act_df = act_df.merge(ac_df, left_on='ID', right_on='BC2.ID').drop(columns=['BC2.ID']).rename(columns={'BarcodeCounts': 'input_reads'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(16,7))\n",
    "sns.scatterplot(\n",
    "    data=act_df,\n",
    "    x='input_reads', y='activity',\n",
    "    hue='variant_rank_bin',\n",
    "    palette='Paired',\n",
    "    s=5, ec=None,\n",
    "    ax=ax[0])\n",
    "sns.scatterplot(\n",
    "    data=act_df,\n",
    "    x='cleaved_reads', y='uncleaved_reads',\n",
    "    hue='activity',\n",
    "    palette='viridis',\n",
    "    s=5, ec=None,\n",
    "    ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simulated, the higher the rank of the variant the more input reads it had, which is proportional to the inferred activity. This is seen in the plot on the left. The right plot shows those high activity variants having a larger proportion of cleaved reads relative to their uncleaved read counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us use a callback function to filter out any reads not matching our simulated criteria of the cleavage location being the 10th base of the variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we will observe a couple of things. First, the cleaved reads will only contain `BC2` in them. Second, the subsequence specified in the `Variant` column will appear partially in the read. Infact as simulated, the cleaved reads will contain part of the variant, the whole of `PinkForwardPrimer` and `BC2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `xcount` and `acount` allows us to use the information from the reads to concurrently do our own calculations on the reads and to signal to the counting algorithm whether to use the read as part of the counting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous CC output, if it exists\n",
    "! rm -f CCC.oligopool.xcount.csv CCC.oligopool.xcount.failed_reads.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a shared dictionary for our callback below (detailed later)\n",
    "ribozyme_memory = Manager().dict(zip(df.ID, df.Variant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback logic\n",
    "def filter_non_cleaved_reads(ribozyme_memory, pink_forward_primer):\n",
    "    '''Outer wrapper to give our method some memory.'''\n",
    "\n",
    "    def wrapped(r1, r2, ID, count, coreid):\n",
    "        '''Inner closure to filter reads.\n",
    "\n",
    "        Note that `xcount` and `acount` callbacks must have this signature (r2 is None for merged reads).\n",
    "        See help(op.acount) or help(op.xcount) for details.'''\n",
    "\n",
    "        # If more than one barcode present, we are not going to do anything with it, because\n",
    "        # it implies an uncleaved read. We only want to use BC2-exclusive cleaved reads.\n",
    "        if ID[0] is None and not ID[1] is None:\n",
    "            # The first ID will be None during `xcount` because we will compute BC1 x BC2\n",
    "\n",
    "            # Extract ribozyme ID\n",
    "            ribozyme_id = ID[1]\n",
    "\n",
    "            # The constant primer must be present for acceptance\n",
    "            reads = [r1] if r2 is None else [r1, r2]\n",
    "            for read in reads:\n",
    "                if pink_forward_primer in read:\n",
    "\n",
    "                    # Extract ribozyme fragment\n",
    "                    ribozyme_fragment = read[:read.index(pink_forward_primer)]\n",
    "                    # Fetch the expected full variant\n",
    "                    ribozyme_full = ribozyme_memory[ribozyme_id]\n",
    "\n",
    "                    # Figure out where the fragment starts in the variant\n",
    "                    if ribozyme_fragment in ribozyme_full:\n",
    "                        ribozyme_start = ribozyme_full.index(ribozyme_fragment)\n",
    "                        # This index must be 10 or greater for cleaved product-based read\n",
    "                        return ribozyme_start >= 10\n",
    "\n",
    "            '''\n",
    "            Note: Here we are using exact matching logic. In reality we might want to do\n",
    "                  some alignment based extraction of our fragments to be error-aware.\n",
    "            '''\n",
    "\n",
    "        # Reject the read for counting ... didn't pass filtering due to one or more reasons\n",
    "        return False\n",
    "\n",
    "    # Return Wrapped Processor\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtually any logic can be executed concurrently with counting, as long as the callback returns a boolean. If a callback function returns `False`, the counting function will not use the reads for counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to, we can also record additional results from our callback calculations via [shared data structures](https://superfastpython.com/multiprocessing-manager-example/) such as a `Manager` `dict` or `list`. This is useful, for example, if we wanted to extract and store the location of cleavage sites in reads (instead of assuming it to be the 10th base in the designed variants). Another similar use case is extracting transcription start sites for a library of designed promoters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** The `Manager` `dict` here ensures that all parallel callback processes during counting will have access to the same exact dictionary, controlled and synchronized by a separate `Manager` instance. This allows us to modify the content of the managed dictionary in one process and have the changes be reflected in callbacks executed by other counting processes in parallel. This also allows us to easily gather and store additional results, in parallel, to a persistent and centralized dictionary for post-counting analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we need to use the outer wrapper as callback\n",
    "cc_callback = filter_non_cleaved_reads(\n",
    "    ribozyme_memory=ribozyme_memory,\n",
    "    pink_forward_primer=df.PinkForwardPrimer.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry, if our callback fails, the return stats dictionary will contain the inputs for which our callback failed and we can debug accordingly. If you'd like to see *examples* of discarded reads (including failures from callbacks, barcode detection, anchoring, low complexity, and PhiX), we can enable `failed_reads_file` to write a small per-category sample to disk, as we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, stats = op.xcount(\n",
    "    index_files=['BC1', 'BC2'],\n",
    "    pack_file='NGS.oligopool.pack',\n",
    "    count_file='CCC',\n",
    "    failed_reads_file='CCC',            # Sample failed reads for diagnostics\n",
    "    failed_reads_sample_size=500,       # Max samples per failure category\n",
    "    mapping_type='sensitive',           # Use sensitive/slow classification\n",
    "    barcode_errors=-1,\n",
    "    callback=cc_callback,\n",
    "    core_count=0,\n",
    "    memory_limit=0.0,\n",
    "    verbose=False) # Silent Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our callback filtering function rejected a lot of reads, as recorded in `callback_false_reads` key in the stats dictionary. Rest of the reads were cleaved and were accepted for counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now investigate our `failed_reads_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_df = pd.read_csv('CCC.oligopool.xcount.failed_reads.csv')\n",
    "fail_df['failure_reason'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our `Oligopool Calculator` `Analysis Mode` walkthrough.\n",
    "\n",
    "**Key takeaways:**\n",
    "- `index` trains a `Scry` classifier on your barcode architecture (include associates for `acount`)\n",
    "- `pack` QC-filters reads and deduplicates them into efficient read packs\n",
    "- Use `acount` to verify barcode-variant associations (synthesis QC, input library distribution)\n",
    "- Use `xcount` to count barcode combinations (activity assays, multi-barcode designs)\n",
    "- Custom callbacks enable concurrent read filtering/processing during counting\n",
    "- Use `Manager` dicts for thread-safe shared state across parallel callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f BC*.oligopool.index NGS.oligopool.pack AC.oligopool.acount.csv CC*.oligopool.xcount.csv CCC.oligopool.xcount.failed_reads.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Oligopool Calculator` offers a comprehensive solution for building and analyzing genetic part libraries using oligo pools. `Design Mode` helps you compose constrained, synthesis-ready architectures; `Assembly Mode` supports post-synthesis fragment assembly for long constructs; `Degenerate Mode` enables cost-efficient selection workflows via IUPAC-degenerate synthesis; and `Analysis Mode` provides fast, flexible barcode-based quantification from NGS reads. We hope it helps you spend less time reinventing pipelines and more time learning biology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlde-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
